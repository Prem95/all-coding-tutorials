{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/netflix-deep-learning-part-2","result":{"data":{"markdownRemark":{"id":"f1b32413-deff-5326-80d2-a1ca7a94532b","html":"<p><img src=\"https://cdn-images-1.medium.com/max/11082/0*X6o4wyXaZxNC9bEa\" alt=\"Photo by [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&#x26;utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&#x26;utm_medium=referral)\"></p>\n<p><a href=\"https://medium.com/analytics-vidhya/tensorflow-for-recommendation-model-part-1-19f6b6dc207d\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Previous Tutorial: TensorFlow for Recommendation Model: Part 1</strong></a></p>\n<p>Let’s continue on where we left off. In the previous series, we had used Matrix Factorization to learn the user embeddings vector to perform the recommendations for us. These user embeddings represent the learned features between the users and the movie matrix called the feedback matrix. Now, let’s understand how Deep Learning could be used in performing the recommendations instead. The Google Colab is linked below.</p>\n<p><a href=\"https://colab.research.google.com/drive/11zEddP5TUAVCTkUgFwtFN7LG1z2A85hB#scrollTo=TOF0qSte0ljl&#x26;uniqifier=4\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Google Colaboratory</strong></a></p>\n<h2 id=\"disadvantages-of-matrix-factorisation\" style=\"position:relative;\"><a href=\"#disadvantages-of-matrix-factorisation\" aria-label=\"disadvantages of matrix factorisation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Disadvantages of Matrix Factorisation</h2>\n<p>For the most part, there are some disadvantages of using MF to perform recommendations:</p>\n<ul>\n<li><strong>Small feature space</strong>: The only features that can be queried if either movie or user id.</li>\n<li><strong>The relevance of recommendation</strong>: Since MF is about using dot product or Euclidean distance, the most popular items are recommended to everyone.</li>\n</ul>\n<h2 id=\"convolutional-neural-network\" style=\"position:relative;\"><a href=\"#convolutional-neural-network\" aria-label=\"convolutional neural network permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Convolutional Neural Network</h2>\n<p>In neural nets, CNN remains the main algorithm to perform image classification, image recognition, object detection, etc.\n<a href=\"https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Understanding of Convolutional Neural Network (CNN) — Deep Learning</strong></a></p>\n<p>In recommendations model, however, we can go about creating a simple softmax CNN model where the parameters are:</p>\n<ul>\n<li><strong>Input</strong>: User query input and the items associated with it.</li>\n<li><strong>Output</strong>: Interaction probability of a user with the items.</li>\n</ul>\n<p>In the input space, more side features can be added, such as watch time, countries, and more. This will allow more data to be learned by the NN in the hidden layer, thus, making more connections in those layers.</p>\n<h2 id=\"activation-layers\" style=\"position:relative;\"><a href=\"#activation-layers\" aria-label=\"activation layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Activation Layers</h2>\n<p>By defining the architecture of the NN, activations layers can be added to capture more hidden connections further and form a better feature sampling in the hidden layers. For this recommendation model, the Rectified Linear Unit (ReLU) will be used.</p>\n<p>In most NN, gradient descent algorithm will be used where during the back-propagation of the errors, an activation function is needed to act as a linear function. This is where ReLU comes in.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2128/1*CWNz_yXHhJA8ssMBNPcz9g.png\" alt=\"Source: [https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\"></p>\n<p>If we take a look at the image on the left, we can see that equation of the function can be represented by <strong><em>y=max(0, x), which</em></strong> means to adopt the values of y when the value of x is greater than 0. This acts as a linear and a non-linear function where the function is linear for amounts greater than zero, meaning it has a lot of the desirable properties of a linear activation function. Yet, it is a non-linear function as negative values are always output as zero. This is why the ReLu activation function is mostly used on modern NN.</p>\n<h2 id=\"softmax-as-the-final-layer\" style=\"position:relative;\"><a href=\"#softmax-as-the-final-layer\" aria-label=\"softmax as the final layer permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Softmax as the Final Layer</h2>\n<p>The model maps the output of the last layer, through a softmax layer to a probability distribution. A function that takes as input a vector of X real numbers, and normalizes it into a probability distribution consisting of X probabilities proportional to the exponentials of the input numbers. To put into the recommendations models perspective:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*nKbDx69gMJYN9v7CsAe3YA.png\"></p>\n<ul>\n<li><strong><em>x</em></strong> represents the individual probability of the user-feature interaction output. This can be the suggested movie that the user will be watching next</li>\n<li><strong>*σ(x)</strong> *represents the probability of that output over a distribution across ALL the outputs in the final layer\n<a href=\"https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Understand the Softmax Function in Minutes</strong></a></li>\n</ul>\n<h2 id=\"embeddings-in-neural-network\" style=\"position:relative;\"><a href=\"#embeddings-in-neural-network\" aria-label=\"embeddings in neural network permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Embeddings in Neural Network</h2>\n<p>Compared to the MF model, the NN model also has embeddings to represent the relationship between the user and the item. The model learns the embeddings, V per item, j.</p>\n<p>The query embeddings, however, are different. Instead of learning one embedding per query, the system determines a mapping from the query feature to an embedding. Therefore, since the activation function (ReLU) is present, the embeddings are now generalized as a non-linear function as compared to the MF model.</p>\n<p>There are possible ways to perform dot product into these embeddings to obtain a final probability score, but it is not part of the scope in this series.</p>\n<h2 id=\"input-data\" style=\"position:relative;\"><a href=\"#input-data\" aria-label=\"input data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Input Data</h2>\n<p>Now let’s define the training data for the created NN model. The training data consists of the query features and a vector of items the user interacted with (represented as a probability distribution). The variables or the weights represent the interaction of the user-item features and are randomly initialized for training.</p>\n<h3 id=\"creating-the-data\" style=\"position:relative;\"><a href=\"#creating-the-data\" aria-label=\"creating the data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Creating the data</h3>\n<p>The input data is created from the ratings() dataFrame and the movies() dataFrame.</p>\n<p>This dataframe represents the basic information we would have to collate from to create the input data</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">genre_cols = [\n    \"genre_unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"]\n\nmovies_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'imdb_url'] + genre_cols\nmovies = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ml-100k/u.item', sep='|', names=movies_cols, encoding='latin-1')</code></pre></div>\n<p>Splitting the train and testing data:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def split_dataframe(df, holdout_fraction=0.1):\n\n  test = df.sample(frac=0.1, replace=False)\n  train = df[~df.index.isin(test.index)]\n\n  return train, test</code></pre></div>\n<p><a href=\"https://www.pythonforbeginners.com/basics/list-comprehensions-in-python\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">List comprehension</a> is used in a way to create the feature since it is much faster, and also it is something that most coders are using nowadays when memory is an issue.</p>\n<h3 id=\"data-batches\" style=\"position:relative;\"><a href=\"#data-batches\" aria-label=\"data batches permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Batches</h3>\n<p>As for the training input data, the data is propagated using batches into the softmax model. A <strong>batch</strong> size is a number of samples processed before the model is updated at the end when the ground truth and the output is compared for another iteration. A batch of 4 features will be propagated into the NN model. This will be the movie_id, year, genre, and the label. Hence, each user will have these particular interactions being learned in the NN model. <strong><em>tf.data.Dataset()</em></strong> is used to initialize the batch operation. Batch size is crucial as a larger batch size (with memory constraints) will allow the NN to learn more effectively at the optimizer stage</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def make_batch(ratings, batch_size):\n\n  # Function to fill null values to form sparse tensor\n  def pad(x, fill):\n    return pd.DataFrame.from_dict(x).fillna(fill).values\n\n  movie = []\n  year = []\n  genre = []\n  label = []\n\n  # Fill the input with 4 features\n  for movie_ids in ratings[\"movie_id\"].values:\n    movie.append(movie_ids)\n    genre.append([x for movie_id in movie_ids for x in genres_dict[movie_ids]])\n    year.append([yearsDict[movie_id] for movie_id in movie_ids])\n    label.append([int(movie_id) for movie_id in movie_ids])\n\n  # Creating the input tensors\n  features = {\n      \"movie_id\": pad(movie, \"\"),\n      \"year\": pad(year, \"\"),\n      \"genre\": pad(genre, \"\"),\n      \"label\": pad(label, -1)\n  }\n\n  # Creating a single batch for each iteraton\n  batch = (\n      tf.data.Dataset.from_tensor_slices(features)\n      .shuffle(1000)\n      .repeat()\n      .batch(batch_size)\n      # one_shot_iterator only in TF1.X\n      .make_one_shot_iterator()\n      .get_next())\n\n  return batch</code></pre></div>\n<h3 id=\"neural-network-training\" style=\"position:relative;\"><a href=\"#neural-network-training\" aria-label=\"neural network training permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Neural Network Training</h3>\n<p>Once the input data and batches are processed, the neural network training can finally begin. An embedding() function is created to capture the features of the training data. A softmax final layer neural network model is used to output the probability distribution consisting of K probabilities proportional to the exponentials of the input numbers.</p>\n<p>Creating the loss function using Softmax:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def softmax_loss(user_embeddings, movie_embeddings, labels):\n\n  # Verify that the embddings have compatible dimensions\n  user_emb_dim = user_embeddings.shape[1].value\n  movie_emb_dim = movie_embeddings.shape[1].value\n  logits = tf.matmul(user_embeddings, movie_embeddings, transpose_b=True)\n  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits=logits, labels=labels))\n  return loss</code></pre></div>\n<p>Performing the model training:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Create feature embedding columns\ndef make_embedding_col(key, embedding_dim):\n  categorical_col = tf.feature_column.categorical_column_with_vocabulary_list(key=key, vocabulary_list=list(set(movies[key].values)), num_oov_buckets=0)\n  return tf.feature_column.embedding_column(categorical_column=categorical_col, dimension=embedding_dim, combiner='mean')\n\n# Initialise graph for training\nwith tf.Graph().as_default():\n  softmax_model = build_softmax_model(\n                  rated_movies,  # Input\n                  embedding_cols=[ # Embeddings learned\n                  make_embedding_col(\"movie_id\", 35),\n                  make_embedding_col(\"year\", 2),],\n                  hidden_dims=[35]) # Dimension of the embedding\n\n# Perform training\nsoftmax_model.train(learning_rate=8., num_iterations=3000, optimizer=tf.train.AdagradOptimizer)</code></pre></div>\n<h2 id=\"final-summary\" style=\"position:relative;\"><a href=\"#final-summary\" aria-label=\"final summary permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Final Summary</h2>\n<p>To put into perspective, oftentimes Matrix Factorisation is used in a large <strong>sparse data</strong> environment where a <strong>simple dot product function</strong> can be used to identify the closest item in relation to the user. This is particularly useful where time and memory is a constraint in the system. It is <strong>easier to scale</strong>, cheaper to query, and much faster.</p>\n<p>As for Neural Network, a more complex relationship can be formed to understand how the recommendations work inside the model. NN model captures more personalized preference since the inputs can be a vector of interactions between the user and say, country, age, watch time and etc. In some cases, NN models are more preferred since the predictions will be more <strong>accurate and more user-oriented/biased</strong> and not purely based on the dot distance between users and items. This is particularly useful for other use cases such as marketing or ads-targeting, where a large number of user data is collected, and NN can understand the <strong>underlying preference</strong> of each user. By also using NN, the query embedding usually needs to be computed at query time; as a result, this can be computationally expensive.</p>\n<h2 id=\"in-the-end\" style=\"position:relative;\"><a href=\"#in-the-end\" aria-label=\"in the end permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>In the end</h2>\n<p>If you managed to retain your attention to this point, please leave a comment if you have any advice for this series, or any experience trying to build a recommendation model using MF of NN.</p>","fields":{"slug":"/posts/netflix-deep-learning-part-2","tagSlugs":["/tag/python/","/tag/deep-learning/","/tag/recommendation-model/"]},"frontmatter":{"date":"2021-09-12","description":"We will be looking at how to build a movie recommendation model using TensorFlow (Part 2)","tags":["Python","Deep Learning","Recommendation Model"],"title":"Netflix Movie Recommendation using Deep Learning (with Neural Network)","socialImage":null}}},"pageContext":{"slug":"/posts/netflix-deep-learning-part-2"}},"staticQueryHashes":["251939775","2549761526","401334301"]}