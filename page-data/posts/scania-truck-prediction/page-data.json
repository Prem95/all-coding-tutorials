{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/scania-truck-prediction","result":{"data":{"markdownRemark":{"id":"6a14dfcc-b098-579e-9be9-ec694b7589cd","html":"<p>In this guide, we would be building a simple classifier to predict if the trucks at Scania Trucks needs to be serviced or not.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/7962/1*e50l4JNveqOuE2hX4Zg_JQ.jpeg\" alt=\"Photo by [Rhys Moult](https://unsplash.com/@rhysatwork?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/truck?utm_source=unsplash&#x26;utm_medium=referral&#x26;utm_content=creditCopyText)\"></p>\n<h2 id=\"outline\" style=\"position:relative;\"><a href=\"#outline\" aria-label=\"outline permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Outline</h2>\n<ul>\n<li>Pre-processing of datasets for training and validation</li>\n<li>Understand the steps to build a classifier</li>\n<li>Comparing the accuracy of the classifiers</li>\n</ul>\n<h2 id=\"objective\" style=\"position:relative;\"><a href=\"#objective\" aria-label=\"objective permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Objective</h2>\n<ul>\n<li>Minimize the servicing cost by correctly predicting of the trucks needs to be serviced or will cause a breakdown in the future</li>\n</ul>\n<h2 id=\"data-processing\" style=\"position:relative;\"><a href=\"#data-processing\" aria-label=\"data processing permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Processing</h2>\n<p>Datasets: <a href=\"https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Link</a></p>\n<p>The training set contains 60000 examples in total in which 59000 belongs to the negative class and the remaining 1000 belongs to be positive class. The test set contains 16000 examples. There are a total of 117 attributes or features that will be used in the classifiers. These attributes are anonymous for obvious reason.</p>\n<p><em>Github: <a href=\"https://github.com/Prem95/DataSciencePortfolio/blob/master/APS%20Failure%20Prediction.ipynb\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Link</a></em></p>\n<p>Once the data is loaded, we can perform some simple EDA to explore the data further. What we did was:</p>\n<ul>\n<li>Convert all ‘neg’ and ‘pos’ values to 0 and 1 respectively</li>\n<li>Assign the class labels with the newly created list of zeros and ones</li>\n</ul>\n<p>Once the data is processed, we get to see that the ‘negative’ classes has a total of 59000 value as compared to ‘positive’ classes at 1000. Hence, our training datasets are highly imbalanced. To solve this, some sampling is required.</p>\n<h2 id=\"imputations\" style=\"position:relative;\"><a href=\"#imputations\" aria-label=\"imputations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Imputations</h2>\n<p>In real world data, missing values are inevitable. This missing values actually carry some important information if its not included in the overall distribution during prediction or training. To solve this, generally we can perform mean, median or most frequent imputations. To get a visual idea of how many null values are there, using the library msno, we can plot the distribution. By a rough estimation, we would want to omit the columns where 0.7% (42000 values) of the data are null values.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2092/1*vl_9ZmT2xR1F7zVEvoYqdg.png\" alt=\"imbalanced-data\">\n  </p><div style=\"width: 200px; text-align: center;\">Before Imputation</div>\n<p></p>\n<p>Once the omit is done, it is time to fill in the NaN values that are left in the columns. To do this, I went with using mean imputation to fill in the NaN values. This will attenuate any correlation involving the variables that are imputed. There is guaranteed to be no relationship between the imputed variable and any other measured variables.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2104/1*fDrWgH_Cu9JEZJxHGbx3xA.png\" alt=\"balanced-data\">\n  </p><div style=\"width: 200px; text-align: center;\">After Imputation</div>\n<p></p>\n<h2 id=\"handling-imbalanced-data\" style=\"position:relative;\"><a href=\"#handling-imbalanced-data\" aria-label=\"handling imbalanced data permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Handling imbalanced data</h2>\n<p>Imbalanced data is one of the most common problem in machine learning. There are several ways on how to access this problem. The most common way to do perform oversampling and undersampling the minority and majority classes. Take a look at the distribution of the NEG vs POS classes in the class column below (a simple <em>value_counts()</em> would be enough):</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*kWTyLyUAQ6OdVsRPkJ0ArQ.png\" alt=\"Distribution of classes\">\n  </p><div style=\"width: 200px; text-align: center;\">Distribution of classes</div>\n<p></p>\n<p>We do see that the overall distribution is very imbalanced. Hence, to solve this we would have to undersample the NEGATIVE classes and oversample the POSITIVE class using SMOTE technique.</p>\n<h2 id=\"synthetic-minority-over-sampling-technique-smote\" style=\"position:relative;\"><a href=\"#synthetic-minority-over-sampling-technique-smote\" aria-label=\"synthetic minority over sampling technique smote permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Synthetic Minority Over-sampling Technique (SMOTE)</h2>\n<p>SMOTE is one of the methods used to oversample the minority classes. Lets dive a bit deeper into the function of SMOTE. SMOTE is part of the imblearn library.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">imblearn.over_sampling.SMOTE(*sampling_strategy='auto'*, *random_state=None*, *k_neighbors=5)</code></pre></div>\n<p>SMOTE works by creating synthetic variables around the minority classes by finding the minimum Euclidean distance between the neighbours bounded by the <em>k</em> value. There are basically 2 hyperparameters that are the K and Ratio, that can be changed to determine the overall function of the SMOTE analysis.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*S4-sTzCAZw_uBHdf85md9A.png\" alt=\"Distribution of classes\">\n  </p><div style=\"width: 900px; text-align: center;\">How the synthetic samples are created</div>\n<p></p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*8DokckllmUw7oqmpmCgDiA.png\" alt=\"k-means-clustoring\">\n  </p><div style=\"text-align: center;\">How k determines the spread of the variables</div>\n<p></p>\n<p>k refers to the vector that will be used to create the synthetic variables. At k = 1 only the closest neighbor of the same class is considered. Not necessarily that as the k increases, more accurate synthetic variables will be created. Ratio refers to ratio of the new data points to be created around the boundary. In most cases, the k and ratio values are set to 5 and 0.1 respectively as defaults.</p>\n<h2 id=\"data-standardisation\" style=\"position:relative;\"><a href=\"#data-standardisation\" aria-label=\"data standardisation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Standardisation</h2>\n<p>Why is data standardisation important? To put it in simple terms, we would like to have all values across the entries to have similar consistency. That is to have the same format and content across the entries. We do not need the ‘amplitude’ of the features, but only the underlying <strong>distribution shape.</strong></p>\n<ol>\n<li>The most commonly standardisation technique used is to de-mean the data and refactoring zero variance:</li>\n</ol>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*5oc5J2ZanfyG4wGbNHejjQ.png\" alt=\"Normalisation of X\">\n  </p><div style=\"text-align: center;\">Normalisation of X</div>\n<p></p>\n<ol start=\"2\">\n<li>MinMax scaling to spread data across 0 and 1</li>\n</ol>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*E9roKxhL_lw8jkvINrHZvA.png\" alt=\"Confusion Matrix\">\n  </p><div style=\"text-align: center;\">Confusion Matrix</div>\n<p></p>\n<p>There are several advantages of performing data normalisation across the data entries. These has been listed below:</p>\n<ul>\n<li><strong>Data is less sensitive to the scale of features</strong>: This is true in a sense where you would have 2 different variables of different scales ie. W1 and W2 where W1 represents an object and W2 represents the area of the object. During a linear regression, the scale of W2 is larger w.r.t W1, and there could be some possibilities where only by tuning W2, the prediction is accurate.</li>\n<li><strong>Consistency for comparing results across models</strong>: This is particularly important where research are actively being done on different state-of-the-art models so scaling will be important</li>\n<li><strong>Speed of convergence</strong>: Convergence speed (Gradient Descent) typically is computed by the eigenvalues (X^T*X). In a typical gradient descent contour, the speed of convergence depends on the ‘Hessian condition number’, that is the ratio of the highest derivative and lowest derivative. If the number if large, the speed will be slow. Having an unscaled features (X and 100X for example), will ultimately increase the condition number.</li>\n</ul>\n<h2 id=\"random-forest-prediction-model\" style=\"position:relative;\"><a href=\"#random-forest-prediction-model\" aria-label=\"random forest prediction model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><strong>Random Forest Prediction Model</strong></h2>\n<p>To understand this, lets look at the basics the binary tree.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2028/1*WcsdF8W8tn52AnTuZmRl3w.png\" alt=\"Simple binary tree\">\n  </p><div style=\"text-align: center;\">Simple binary tree</div>\n<p></p>\n<p>In a tree, there is the root node (Height) and multiple leaf nodes (Weight, etc). The leaf nodes of a tree contain an output variable (y) which will be used to make the prediction. In the decision tree, the questions are answered accordingly, in which transversal happens (node splitting) until the <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Gini Impurity</a> is reduced. In simple terms, the tree tries to form (with the help of GI) high number of sample where it can be split by classes.</p>\n<p>Why random forest or multiple forest models? A random forest is made up of multiple decision trees where the training data and features are sampled randomly across the trees and nodes respectively.</p>\n<p>The random forest will combines large number of single decision trees and will perform the training on dataset that are slightly different of observations. The final prediction is computed by averaging the predictions of each single tree. However, <a href=\"https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">feature importance</a> is considered important during node splitting as it will affect the Gini Impurity. Other hyper-parameters can also be tuned by performing cross validation (Grid Search) to find the optimum hyper-params.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2780/1*b_5Ip7_hLRviAUS-Sa_orQ.png\" alt=\"A single tree in a random forest model\">\n  </p><div style=\"text-align: center;\">A single tree in a random forest model</div>\n<p></p>\n<p>In our model, a random forest classifier is used:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">clf = ens.RandomForestClassifier(n_estimators = 100, max_depth = 100, n_jobs = -1)\nclf.fit(trainStandard, y_train)</code></pre></div>\n<p><em>n_estimators</em> and <em>max_depth</em> are considered a hyper-parameters and should be cross validated to find the optimum values.</p>\n<h2 id=\"confusion-matrix\" style=\"position:relative;\"><a href=\"#confusion-matrix\" aria-label=\"confusion matrix permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Confusion Matrix</h2>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*E9roKxhL_lw8jkvINrHZvA.png\" alt=\"Scaling using Min and Max values\">\n  </p><div style=\"text-align: center;\">Scaling using Min and Max values</div>\n<p></p>\n<p><a href=\"https://machinelearningmastery.com/confusion-matrix-machine-learning/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Confusion matrix</a> is typically used to describe the performance of a classification model. The prediction matrix is split into predicted class and actual class where the items are <strong>True Positive</strong>, <strong>True Negative</strong>, <strong>False Negative</strong> and <strong>True Negative</strong>.</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*xYoQLgv77Qk5BnIcdv1JoA.png\" alt=\"Confusion Matrix\">\n  </p><div style=\"text-align: center;\">Confusion Matrix</div>\n<p></p>\n<p>Once the model is trained using Random Forest Prediction, the model can be validated using the unseen test datasets and a confusion matrix can be used to predict the accuracy and the cost.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">y_pred = clf.predict(testStandard)\nconMat = sklearn.confusion_matrix(y_test, y_pred)</code></pre></div>\n<p>We can then lastly complete the cost prediction by calculating it according to the business goal above. Our final cost was found to be $12570.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conMat[1][1]*10 + conMat[1][0]*500</code></pre></div>\n<h2 id=\"what-is-next\" style=\"position:relative;\"><a href=\"#what-is-next\" aria-label=\"what is next permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What is next?</h2>\n<p>What other methods can we try to further optimise the cost (reduce cost function)</p>\n<ul>\n<li>Dimensionality reduction using Principal Component Analysis (PCA)</li>\n<li>Dimensionality reduction using Singular Value Decomposition (SVD)</li>\n<li>Over sampling using <a href=\"https://towardsdatascience.com/adasyn-adaptive-synthetic-sampling-method-for-imbalanced-data-602a3673ba16\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Adaptive Synthetic</a> (ADASYN)</li>\n<li>Perform Cross Validation to find optimal parameters for Random Forest Prediction</li>\n</ul>\n<p>Originally published by <a href=\"https://www.premstroke.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Prem Kumar</a> on <a href=\"https://medium.com/analytics-vidhya/scania-trucks-cost-prediction-fdcf8dc22b5b\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Medium</a></p>","fields":{"slug":"/posts/scania-truck-prediction","tagSlugs":["/tag/python/","/tag/data-science/","/tag/recommendation-model/"]},"frontmatter":{"date":"2021-09-12","description":"In this guide, we would be building a simple classifier to predict if the trucks at Scania Trucks needs to be serviced or not..","tags":["Python","Data Science","Recommendation Model"],"title":"Scania Trucks Cost Prediction using Machine Learning","socialImage":null}}},"pageContext":{"slug":"/posts/scania-truck-prediction"}},"staticQueryHashes":["251939775","2549761526","401334301"]}