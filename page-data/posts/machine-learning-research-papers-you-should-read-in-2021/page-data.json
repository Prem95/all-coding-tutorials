{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/machine-learning-research-papers-you-should-read-in-2021","result":{"data":{"markdownRemark":{"id":"add9c35c-8bd8-59c7-ae5b-f7b6be7376bc","html":"<p><img src=\"https://unsplash.com/photos/zbLW0FG8XU8/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MjZ8fGFpfGVufDB8fHx8MTYzNDk0Nzk3NQ&#x26;force=true\" alt=\"Photo by [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&#x26;utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&#x26;utm_medium=referral)\"></p>\n<p><a href=\"https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Machine Learning</a> suddenly became one of the most critical domains of Computer Science and just about anything related to Artificial Intelligence.</p>\n<p>Every company is applying <a href=\"https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Machine Learning</a> and developing products that take advantage of this domain to solve their problems more efficiently.</p>\n<p>Every year, 1000s of research papers related to <a href=\"https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Machine Learning</a> are published in popular publications like NeurIPS, ICML, ICLR, ACL, and MLDS.</p>\n<h2 id=\"deep-residual-learning-for-image-recognition\" style=\"position:relative;\"><a href=\"#deep-residual-learning-for-image-recognition\" aria-label=\"deep residual learning for image recognition permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deep Residual Learning for Image Recognition</h2>\n<p>Arvix: <a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Paper Link</a></p>\n<h3 id=\"abstract\" style=\"position:relative;\"><a href=\"#abstract\" aria-label=\"abstract permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstract:</h3>\n<p><em>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.</em></p>\n<h3 id=\"driven-by-the-significance-of-depth-a-question-arises-is-learning-better-networks-as-easy-as-stacking-more-layers\" style=\"position:relative;\"><a href=\"#driven-by-the-significance-of-depth-a-question-arises-is-learning-better-networks-as-easy-as-stacking-more-layers\" aria-label=\"driven by the significance of depth a question arises is learning better networks as easy as stacking more layers permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers?</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*Fk1gxB6Nl4WRjOMa2xzpCQ.png\" alt=\"Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer plain” networks.\"><em>Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer plain” networks.</em></p>\n<p>Learning the identity function is <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">extremely difficult</a> as the scope of all possible combination of weights and biases is enormous, thus the chance of learning the <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">identity function is minuscule</a>. As seen above, <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">adding more layers</a> to a neural network can actually do the opposite: more layers = lower accuracy (diminishing returns).</p>\n<p>The paper identifies that there is one solution to this. That is by <strong>adding the inputs of the hidden layers to the output</strong></p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*6g78uAVCViTGNaq8uhTcOg.png\" alt=\"Residual learning: a building block.\"><em>Residual learning: a building block.</em></p>\n<p>By implementing this idea on deeper networks, they are able to obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &#x26; COCO 2015 competitions, where they also won the <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation</a>.</p>\n<h2 id=\"single-headed-attention-rnn-stop-thinking-with-your-head\" style=\"position:relative;\"><a href=\"#single-headed-attention-rnn-stop-thinking-with-your-head\" aria-label=\"single headed attention rnn stop thinking with your head permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single Headed Attention RNN: Stop Thinking With Your Head</h2>\n<p>Arvix: <a href=\"https://arxiv.org/pdf/1911.11423.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Paper Link</a>\nAuthor: <a href=\"https://scholar.google.com/citations?hl=en&#x26;user=AolIi4QAAAAJ&#x26;view_op=list_works&#x26;sortby=pubdate\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Steven Merity</a></p>\n<p>In this paper, the Harvard grad Steven Merity introduces a state-of-the-art NLP model called as Single Headed Attention RNN or SHA-RNN. Stephen Merity, an independent researcher that is primarily focused on <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Machine Learning, NLP and Deep Learning</a>. The author demonstrates by taking a simple LSTM model with SHA to achieve a state-of-the-art byte-level language model results on enwik8.</p>\n<p>The author’s primary goal is to show that the entire field might have evolved in a different direction if we had instead been obsessed with a slightly different acronym and somewhat different results.</p>\n<p>The central concept of the model architecture proposed by Steven consists of a <strong>LSTM architecture with a SHA based network</strong> with three variables (Q, K and V).</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2228/1*FO-HeUI06ixQvfuPhIlawg.png\" alt=\"Source: Arvix ([https://arxiv.org/pdf/1911.11423.pdf](https://arxiv.org/pdf/1911.11423.pdf))\"> Source: Arvix (<a href=\"https://arxiv.org/pdf/1911.11423.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1911.11423.pdf</a>)</p>\n<p>Each SHA-RNN layer contains only a single head of attention that helps with keeping the memory consumption of the model to the minimum by eliminating the need to update and maintain multiple matrices.</p>\n<p>The Boom layer is related strongly to the <strong><a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">large feed-forward layer found in Transformers</a></strong> and other architectures. This block reduces and removes an entire matrix of parameters compared to traditional down-projection layers by using Gaussian Error Linear Unit (GeLu) multiplication to break down the input to minimize computations.</p>\n<p>Let’s look at the actual comparison below. In 2016, <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">The Surprisal-Driven Zoneout</a>, a regularization method for RNN, achieved an outstanding compression score of 1.313bpc on the Hutter Prize dataset, <em>enwiki8</em> which is a one-hundred-megabyte file of Wikipedia pages.</p>\n<p>The SHA-RNN managed to achieve even lower (bpc) compared to the model in 2016. That is impressive. Bits per character is a model proposed by Alex Graves to approximate the <a href=\"https://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">probability distribution of the next character</a> given past characters.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*KA-q2POhSnClyZKgcM95Cg.png\" alt=\"Source: [https://arxiv.org/pdf/1911.11423.pdf](https://arxiv.org/pdf/1911.11423.pdf)\"><em>Source: <a href=\"https://arxiv.org/pdf/1911.11423.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/pdf/1911.11423.pdf</a></em></p>\n<p>Further on, the Single Headed Attention RNN (SHA-RNN) managed to achieve strong state-of-the-art results with next to no hyper-parameter tuning and by using a single Titan V GPU workstation. And also, his work has undergone no intensive hyper-parameter tuning and lived entirely on a commodity desktop machine that made the author’s small studio apartment a bit too warm to his liking. Now that’s the passion for <a href=\"https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Machine Learning</a>.</p>\n<h2 id=\"efficientnet-rethinking-model-scaling-for-convolutional-neural-networks\" style=\"position:relative;\"><a href=\"#efficientnet-rethinking-model-scaling-for-convolutional-neural-networks\" aria-label=\"efficientnet rethinking model scaling for convolutional neural networks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</h2>\n<p>Arvix: <a href=\"https://arxiv.org/abs/1905.11946\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Paper Link</a>\nAuthor: <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Tan%2C+M\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Mingxing Tan</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Le%2C+Q+V\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Quoc V. Le</a></p>\n<p>In this paper, the authors systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. A new scaling method that uniformly scales all dimensions of depth, width and resolution using a simple yet highly effective compound coefficient is demonstrated in this paper.</p>\n<p>The papers propose a simple yet effective compound scaling method described below:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/3592/1*M9pbS5OyhGLczKcuJf8sEQ.png\" alt=\"Source: Arvix ([*https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946))*\"><em>Source: Arvix ([</em><a href=\"https://arxiv.org/abs/1905.11946\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/1905.11946</a>](<a href=\"https://arxiv.org/abs/1905.11946\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://arxiv.org/abs/1905.11946</a>))**</p>\n<p>A network that goes through dimensional scaling (width, depth or resolution) improves accuracy. But the caveat is that the model accuracy drops with larger models. Hence, it is critical to balance all three dimensions of a network (width, depth, and resolution) during CNN scaling for getting improved accuracy and efficiency.</p>\n<p>The <strong>compound scaling method</strong> as above consistently improves model accuracy and efficiency for scaling up existing models such as <a href=\"https://arxiv.org/abs/1704.04861\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">MobileNet</a> (+1.4% Image Net accuracy), and <a href=\"https://arxiv.org/abs/1512.03385\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">ResNet</a> (+0.7%), compared to conventional scaling methods</p>\n<p>Scaling doesn’t change the layer operations; instead, they obtained their base network by doing a Neural Architecture Search* *(NAS) that optimizes for both accuracy and FLOPS. The scaled EfficientNet models consistently reduce parameters and FLOPS by an order of magnitude (up to 8.4x parameter reduction and up to 16x FLOPS reduction) than existing ConvNets such as ResNet-50 and DenseNet-169.</p>\n<p>EfficientNets also achieved state-of-the-art accuracy in 5 out of the eight datasets, such as <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">CIFAR-100</a> (91.7%) and <a href=\"http://www.robots.ox.ac.uk/~vgg/data/flowers/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Flowers</a> (98.8%), with an order of magnitude fewer parameters (up to 21x parameter reduction), suggesting that the EfficientNets also transfers well.</p>\n<h2 id=\"deep-double-descent-where-bigger-models-and-more-data-hurts\" style=\"position:relative;\"><a href=\"#deep-double-descent-where-bigger-models-and-more-data-hurts\" aria-label=\"deep double descent where bigger models and more data hurts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deep Double Descent: Where Bigger Models and More Data Hurts</h2>\n<p>Arvix: <a href=\"https://arxiv.org/abs/1912.02292\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Paper Link</a>\nAuthor: <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nakkiran%2C+P\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Preetum Nakkiran</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Kaplun%2C+G\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Gal Kaplun</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Bansal%2C+Y\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Yamini Bansal</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yang%2C+T\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Tristan Yang</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Barak%2C+B\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Boaz Barak</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Sutskever%2C+I\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Ilya Sutskever</a></p>\n<p>In this paper, the authors at <a href=\"https://openai.com\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">OpenAI</a> defines the effective model complexity <em>(EMC)</em> of a training procedure of a <a href=\"https://towardsdatascience.com/3-machine-learning-research-papers-you-should-read-in-2020-9b639bd0b8f0\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Neural Network</a> as the maximum number of samples on which it can achieve close to zero training error. The experiments that were conducted suggests that there is a critical interval around the <em>interpolation threshold</em>.</p>\n<p>Interpolation threshold means that the model is varied across the number of model parameters, the length of training, the amount of label noise in the distribution, and the number of train samples. The critical region is simply a small region between the under and over-parameterized risk domains.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2640/1*NMI4UpMca_ilDLkfMRpONw.png\" alt=\"Source: [https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent](https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent)\"><em>Source: <a href=\"https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent</a></em></p>\n<p>In most research, the *bias-variance trade-off *is a fundamental concept in classical statistical learning theory. The idea is that models of higher complexity have lower bias but higher variance.</p>\n<h3 id=\"once-a-model-complexity-passes-the-critical-interval-models-overfit-with-the-variance-term-dominating-the-test-error-and-hence-from-this-point-onward-increasing-model-complexity-will-only-decrease-performance-called--double-descent-phenomenon\" style=\"position:relative;\"><a href=\"#once-a-model-complexity-passes-the-critical-interval-models-overfit-with-the-variance-term-dominating-the-test-error-and-hence-from-this-point-onward-increasing-model-complexity-will-only-decrease-performance-called--double-descent-phenomenon\" aria-label=\"once a model complexity passes the critical interval models overfit with the variance term dominating the test error and hence from this point onward increasing model complexity will only decrease performance called  double descent phenomenon permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Once a model complexity passes the critical interval, models overfit with the variance term dominating the test error, and hence from this point onward, increasing model complexity will only decrease* *performance called — double-descent phenomenon.</h3>\n<p><a href=\"https://openai.com/blog/deep-double-descent/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Deep Double Descent</strong>\n*We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves, then……</a></p>\n<p>The paper defines where three scenarios where the performance of the model reduces as these regimes below becomes more significant.</p>\n<h3 id=\"model-wise-double-descent--larger-model-hurts\" style=\"position:relative;\"><a href=\"#model-wise-double-descent--larger-model-hurts\" aria-label=\"model wise double descent  larger model hurts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(Model-Wise) Double Descent — Larger Model Hurts</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/2196/1*gb59rC2GD3kORri1Zr54lA.png\" alt=\"**Model Regime** (Source: [*https://arxiv.org/abs/1912.02292](https://arxiv.org/abs/1912.02292))*\"><strong>Model Regime</strong> (Source: <a href=\"https://arxiv.org/abs/1912.02292\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">*https://arxiv.org/abs/1912.02292</a>)</p>\n<p>The papers demonstrate model-wise double descent occurrence across different architectures, datasets, optimizers, and training procedures.</p>\n<p>The paper concludes that with the usual modifications that are performed on the dataset before training (e.g., adding label noise, using data augmentation, and increasing the number of train samples), there is a <strong>shift in the peak in test error towards larger models</strong>.</p>\n<p>Also, in the chart above, the peak in test error occurs around the interpolation threshold, when the models are just barely large enough to fit the train set.</p>\n<h3 id=\"sample-wise-non-monotonicity--more-data-hurts\" style=\"position:relative;\"><a href=\"#sample-wise-non-monotonicity--more-data-hurts\" aria-label=\"sample wise non monotonicity  more data hurts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(Sample-Wise) Non Monotonicity — More Data Hurts</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/2072/1*6wOLe0Ba-hUriUMkDtNkxw.png\" alt=\"**Sample Regime** (Source: [*https://arxiv.org/abs/1912.02292](https://arxiv.org/abs/1912.02292))*\"><strong>*Sample Regime</strong> (Source: <a href=\"https://arxiv.org/abs/1912.02292\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">*https://arxiv.org/abs/1912.02292</a>)**</p>\n<p>In this section, the chart shows the effect of varying the number of training samples for a fixed model. Increasing the number of samples shifts the curve downwards towards lower test error but also shifts the peak error to the right.</p>\n<h3 id=\"epoch-wise-double-descent--higher-epoch-hurts\" style=\"position:relative;\"><a href=\"#epoch-wise-double-descent--higher-epoch-hurts\" aria-label=\"epoch wise double descent  higher epoch hurts permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(Epoch-Wise) Double Descent — Higher Epoch Hurts</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/2016/1*NbcZFMAhvvOEpzW_vFHHtw.png\" alt=\"**Epoch Regime** (Source: [*https://arxiv.org/abs/1912.02292](https://arxiv.org/abs/1912.02292))*\"><strong>*Epoch Regime</strong> (Source: <a href=\"https://arxiv.org/abs/1912.02292\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">*https://arxiv.org/abs/1912.02292</a>)**</p>\n<p>For a given number of optimization steps (fixed y-coordinate), test and train error exhibit model-size double descent. For a given model size as the training process proceeds, test and train error decreases, increases, and decreases again; we call this phenomenon epoch-wise double descent.</p>\n<h2 id=\"increasing-the-training-time-increases-the-emc-thus-a-sufficiently-large-model-transitions-from-under-to-over-parameterized-throughout-the-full-training-phase\" style=\"position:relative;\"><a href=\"#increasing-the-training-time-increases-the-emc-thus-a-sufficiently-large-model-transitions-from-under-to-over-parameterized-throughout-the-full-training-phase\" aria-label=\"increasing the training time increases the emc thus a sufficiently large model transitions from under to over parameterized throughout the full training phase permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Increasing the training time increases the EMC, thus a sufficiently large model transitions from under to over-parameterized throughout the full training phase.</h2>\n<p>Further on, larger models with more width parameters such as the ResNet architecture can undergo a significant double descent behavior where the test error first decreases (<em>faster than other size models</em>) then increases near the interpolation threshold, and then decreases again as seen below.</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*V03NIG5cOMJv8xpOENEJcg.png\" alt=\"Source: [https://mltheory.org/deep.pdf](https://mltheory.org/deep.pdf)\"><em>Source: <a href=\"https://mltheory.org/deep.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://mltheory.org/deep.pdf</a></em></p>\n<p>For models at the interpolation threshold, there is effectively only one global model that fits the train data — and forcing it to fit even with small misspecified labels will destroy its global structure. The paper then concludes that there are <em>no good models</em> which both interpolate the train set and perform well on the test set.</p>\n<p>The characterization of these critical regimes, as stated above, provides a useful way of thinking for practitioners, <strong>hopefully, to give a breakthrough in Machine Learning soon.</strong></p>","fields":{"slug":"/posts/machine-learning-research-papers-you-should-read-in-2021","tagSlugs":["/tag/research/"]},"frontmatter":{"date":"2021-11-02","description":"Curated from hundreds of high-quality ML research papers, these are the ones that stood out the most","tags":["Research"],"title":"Top 4 Important Machine Learning and Deep Learning Papers You Should Read in 2021","socialImage":null}}},"pageContext":{"slug":"/posts/machine-learning-research-papers-you-should-read-in-2021"}},"staticQueryHashes":["251939775","2549761526","401334301"]}