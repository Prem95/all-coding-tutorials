{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/netflix-deep-learning-part-1","result":{"data":{"markdownRemark":{"id":"d4547f90-1130-5802-8213-ffa6f7cebf92","html":"<p><img src=\"https://cdn-images-1.medium.com/max/9792/0*VaJk10rzIqhfT-sl\" alt=\"Photo by [Henry &#x26; Co.](https://unsplash.com/@hngstrm?utm_source=medium&#x26;utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&#x26;utm_medium=referral)\"></p>\n<h2 id=\"datasets\" style=\"position:relative;\"><a href=\"#datasets\" aria-label=\"datasets permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Datasets</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">users_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\nusers = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ml-100k/u.user', sep='|', names=users_cols)\n\nratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('/content/drive/My Drive/Colab Notebooks/ml-100k/u.data', sep='\\t', names=ratings_cols, encoding='latin-1')</code></pre></div>\n<h2 id=\"what-is-a-recommendation-model\" style=\"position:relative;\"><a href=\"#what-is-a-recommendation-model\" aria-label=\"what is a recommendation model permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What is a recommendation model?</h2>\n<p>A recommendation model, in simple terms, is an algorithm which aims to provide the most relevant and relatable information to a user depending on the behaviour of the user. Companies like Netflix and Google have a huge database of the behaviours of data collected to be able to perform state-of-the-art recommendations so that they can display the most relevant content or services to the users to increase the engagement. In part 1, we would be building a recommendation model using <strong><em>Collaborative Filtering</em></strong> to recommend movies to users.</p>\n<p><a href=\"https://colab.research.google.com/drive/1GV4lg3LRN-ghtAwJbN_Xy9tQmpAEykPY\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Google Colab Notebook</strong></a></p>\n<h2 id=\"collaborative-filtering\" style=\"position:relative;\"><a href=\"#collaborative-filtering\" aria-label=\"collaborative filtering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Collaborative Filtering?</h2>\n<p>Collaborative filtering tackles the similarities between the users and items to perform recommendations. Meaning that the algorithm constantly find the relationships between the users and in-turns does the recommendations. The algorithm learn the embeddings between the users without having to tune the features. The most common technique is by performing Matrix Factorization to find the embeddings or features that makes up the interest of a particular user.</p>\n<h3 id=\"matrix-factorization\" style=\"position:relative;\"><a href=\"#matrix-factorization\" aria-label=\"matrix factorization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Matrix Factorization</h3>\n<p>Matrix factorization is an embedding. Say we have a user-movie matrix or <em>feedback matrix, A</em>ᴺᴹ, the models learns to decompose into:</p>\n<ul>\n<li>An <strong><em>user</em></strong> embedding vector U, where row N is the embedding for item M.</li>\n<li>An <strong><em>item</em></strong> embedding vector V, where row M is the embedding for item N</li>\n</ul>\n<p>The embedding vector is learned such that by performing UVᵀ, an approximation of the feedback matrix, A can be formed.</p>\n<h3 id=\"loss-function\" style=\"position:relative;\"><a href=\"#loss-function\" aria-label=\"loss function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Loss Function</h3>\n<p>To approximate the feedback matrix, a loss function is needed.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">predictions = tf.gather_nd(tf.matmul(vectorU, vectorV, transpose_b=True),sparse_matrix_UV.indices)\nloss_mse = tf.losses.mean_squared_error(sparse_matrix_UV.values, predictions)</code></pre></div>\n<p>One of the intuitive loss function is using mean squared error (MSE). MSE computes the difference in the feedback matrix A and the approximated UVᵀ matrix. In simple terms:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*v4Iv2GF8_QXt7j6Hn9ENfA.png\"></p>\n<p>where:</p>\n<ul>\n<li><strong><em>n</em></strong> represents the total number of users</li>\n<li><strong><em>Yᵢ</em></strong> represents the feedback matrix A</li>\n<li><strong><em>Yᵢ_bar</em></strong> represents the approximated matrix UVᵀ</li>\n</ul>\n<h3 id=\"regularization-function\" style=\"position:relative;\"><a href=\"#regularization-function\" aria-label=\"regularization function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Regularization Function</h3>\n<p>One of the most common problems with training the model is overfitting. Overfitting happens because the model is trying to learn the embedding of certain features that does not contribute to the accuracy of the model. If this particular outlier feature has large ‘amplitude’ or bias, then it is said that the model is over-fitted to these particular features. To reduce that, we can add a penalty term to the MSE loss. The most common models used are <strong>Lasso Regression</strong> and <strong>Ridge Regression</strong>.</p>\n<p><a href=\"https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a\" target=\"_blank\" rel=\"nofollow noopener noreferrer\"><strong>Regularization in Machine Learning</strong>\n<em>One of the major aspects of training your machine learning model is avoiding overfitting. The model will have a low…</em>towardsdatascience.com</a></p>\n<p><strong>Ridge Regression (L2)</strong></p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*4Ip1dld4EDwR-_9MT_C0tA.png\" alt=\"Ridge Formula\">\n  </p><div style=\"width: 200px; text-align: center;\">Ridge Formula</div>\n<p></p>\n<ul>\n<li>Takes the <strong>square</strong> of the coefficient, <strong><em>w</em></strong>. Hence, any large <strong><em>w</em></strong> values will be penalized to reduce overfitting.</li>\n</ul>\n<p><strong>Lasso Regression (L1)</strong></p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*lJ0IFi99QIM2XpcT8FJE0g.png\" alt=\"Lasso Formula\">\n  </p><div style=\"width: 200px; text-align: center;\">Lasso Formula</div>\n<p></p>\n<ul>\n<li>Takes the <strong>magnitude</strong> of the coefficient, <strong><em>w</em></strong>. Hence, any large <strong>w</strong> values will be penalized to reduce overfitting.</li>\n<li>Useful to shrink the coefficients and perform feature selection if you have a sparse matrix, which is what we need.</li>\n</ul>\n<h2 id=\"making-recommendations\" style=\"position:relative;\"><a href=\"#making-recommendations\" aria-label=\"making recommendations permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Making recommendations</h2>\n<p>The training model can be created as below:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">class Model(object):\n\n  def __init__(self, embedding_vars, loss, metrics=None):\n\n    self.embedding_vars = embedding_vars\n    self.loss = loss\n    self.metrics = metrics\n    self.embeddings = {k: None for k in embedding_vars}\n    self.session = None\n\n  def embeddings(self):\n\n    return self.embeddings\n\n  def train(self, num_iterations=100, learning_rate=1.0, plot_results=True, optimizer=tf.train.GradientDescentOptimizer):\n\n    with self.loss.graph.as_default():\n\n      # Minimize loss function\n      train_op = optimizer(learning_rate).minimize(self.loss)\n\n      # Initialise the operation\n      local_init_op = tf.group(tf.variables_initializer(optimizer(learning_rate).variables()), tf.local_variables_initializer())\n\n      if self.session is None:\n        self.session = tf.Session()\n\n        with self.session.as_default():\n          self.session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n\n    with self.session.as_default():\n      local_init_op.run()\n      iterations = []\n      metrics = {}\n      metrics_vals = {}\n\n      # Train and append results.\n      for i in range(num_iterations + 1):\n        _, results = self.session.run((train_op, metrics))\n        if i == num_iterations:\n\n          for metric_val, result in zip(metrics_vals, results):\n\n            # Embeddings are u and k respectively\n            for k, v in result.items():\n              metric_val[k].append(v)\n\n      for k, v in self.embedding_vars.items():\n        self.embeddings[k] = v.eval()</code></pre></div>\n<p>Add regularization function L1 to reduce overfitting of outlier features</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def build_model(ratings, embedding_dim=3, regularization_coeff=0.1, init_stddev=0.1):\n\n  # Split the training and test datasets\n  train_ratings, test_ratings = split_dataframe(ratings)\n\n  # Creating a random variable vector for the embeddings\n  vectorU = tf.Variable(tf.random.normal([build_rating_sparse_tensor(train_ratings).dense_shape[0], embedding_dim], stddev=init_stddev))\n  vectorV = tf.Variable(tf.random.normal([build_rating_sparse_tensor(train_ratings).dense_shape[1], embedding_dim], stddev=init_stddev))\n\n  # Computing the mean squared error for feedback matrix approximation\n  train_loss = mse_error(build_rating_sparse_tensor(train_ratings), vectorU, vectorV)\n  test_loss = mse_error(build_rating_sparse_tensor(test_ratings), vectorU, vectorV)\n\n  # Buiding the cost function for L1 regularization.\n  feedbackLoss = train_loss + (regularization_coeff * (tf.math.reduce_sum(tf.math.abs(vectorU))/vectorU.shape[0].value +\n                                                      tf.math.reduce_sum(tf.math.abs(vectorV))/vectorV.shape[0].value))\n  loss_components = {'loss': feedbackLoss}\n\n  embeddings = {\"user_id\": vectorU, \"movie_id\": vectorV}\n\n  return Model(embeddings, feedbackLoss, [loss_components])</code></pre></div>\n<p>Generally, the steps <em>(and functions)</em> are listed below:</p>\n<ul>\n<li>Create a <strong>sparse tensor</strong>: <em>tf.SparseTensor()</em>, for <strong><em>U</em></strong> and <strong><em>V</em></strong> matrix with random initialisation</li>\n<li>Create the <strong>loss function and optimiser</strong>: <em>tf.losses.mean<em>squared</em>error()</em> to estimate the total loss with regularization penalty and SGD as the optimiser</li>\n<li>Create the <strong>model</strong>: <em>tf.Session()</em>, Initialise hyperparams, learning rate and embeddings</li>\n<li><strong>Train</strong> the model: <em>tf.Session.run()</em>, to learn the embeddings of the feedback matrix and return the <strong><em>v</em></strong> and <strong><em>k</em></strong> as the embedding vector</li>\n<li><strong>Show</strong> recommendations: <em>df.DataFrame()</em>, to show the closest movie with respect to the user queried</li>\n</ul>\n<p>With the model ready, lets try to compute the recommedations for user ID: <strong>500</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">def userRecommendation(model, userID):\n\n  # Upon getting the user and movie embedding vector, the score recreates the feedback matrix.\n  userEmbedding = model.embeddings['user_id'][userID]\n  movieEmbedding = model.embeddings['movie_id']\n\n  scores = userEmbedding.dot(movieEmbedding.T)\n\n  df = pd.DataFrame({\n      'SCORE': list(scores),\n      'MOVIE': movies['movie_id'],\n      'TITLE': movies['title'],})\n\n  return display.display(df.sort_values('SCORE', ascending=False).head(5)), df</code></pre></div>\n<p>Below is the result:</p>\n<p align=\"center\">\n  <img src=\"https://cdn-images-1.medium.com/max/2000/1*t3-0VJySPHlwZOcvs1p0Bw.png\" alt=\"Final Results\">\n</p>\n<p>We can to see that the model has learned the embeddings for user 500 and it recommends the top 5 movies are User ID: 500 might enjoy.</p>","fields":{"slug":"/posts/netflix-deep-learning-part-1","tagSlugs":["/tag/python/","/tag/deep-learning/","/tag/recommendation-model/"]},"frontmatter":{"date":"2021-09-12","description":"We will be looking at how to build a movie recommendation model using TensorFlow (Part 1)","tags":["Python","Deep Learning","Recommendation Model"],"title":"Netflix Movie Recommendation using Deep Learning (with Collaborative Filtering)","socialImage":null}}},"pageContext":{"slug":"/posts/netflix-deep-learning-part-1"}},"staticQueryHashes":["251939775","2549761526","401334301"]}